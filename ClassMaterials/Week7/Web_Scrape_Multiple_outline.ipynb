{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping multiple pages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have practiced web scraping when all the information we wanted was on a single table of a site. What happens when we want to scrape information from multiple pages?\n",
    "\n",
    "## First example - IMDB \n",
    "\n",
    "Go to https://www.imdb.com/search/title/ and enter the following parameters, leaving all other fields blank or with its default value:\n",
    "\n",
    "- Title Type: Feature film\n",
    "\n",
    "- Release date: From 1990 to 1992\n",
    "\n",
    "- User Rating: 7.5 to \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page you get should be familiar. There's a list with movies and each movie has its title, release year, crew, etc. You could inspect the page and build the code to collect the date.\n",
    "\n",
    "Note the resulting query obtained contain hundreds of movies, and each page only contains 50 of them (you can change the settings to obtain up to 250 movies/page, but that still won't be the complete list).\n",
    "\n",
    "One way to automatize multi page web scraping is to look at the URLs. \n",
    "\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,\n",
    "\n",
    "Note what the url looks like if you scroll down and click on \"Next\", the URL is now: \n",
    "\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\n",
    "\n",
    "Can you see the pattern?\n",
    "\n",
    "our search options are in the parameters title_type, release_date and user_rating. Then, we have the start parameter, which jumps in intervals of 50, and the ref_ parameter, which takes the value of \"adv_nxt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  url: this time, start with the 'second' page\n",
    "url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download html with a request, check response code \n",
    "response=requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  parse html (create the 'soup')\n",
    "soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# check that the html code looks as expected \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll have to build a list of values which jumps by 50, up to the total number of movies we want to scrape.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define iterations \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the iterations work\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the url string for the page search, populate with the iterations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the urls \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respectful scraping:\n",
    "\n",
    "Before starting with the actual scraping, though, there's something we need to note when sending automated requests to websites: it's good practice to let a few seconds pass in between requests. \n",
    "\n",
    "Some pages don't like being scraped and will block your IP if they detect you are sending automated requests. Others might have a small server for the traffic they handle, and sending too many requests might crash the site.\n",
    "\n",
    "The sleep module will help us with that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make it more \"human\", we can randomize the waiting time:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the script to send and store multiple requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ingredients for our multi page scraper : \n",
    "    + iterations \n",
    "    + url list with iterations \n",
    "    + sleepy time + random gaps (to look human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages =[]\n",
    "#assemble urls\n",
    "for i in iterations:\n",
    "    start_at =str(i)\n",
    "    url=\"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=\" + start_at + \"&ref_=adv_nxt\"\n",
    "#download html with get request\n",
    "    response = requests.get(url)\n",
    "#monitor the status codes for each page \n",
    "    print(\"status=\" +str(response.status_code))\n",
    "#store pages into a list \n",
    "    pages.append(response)\n",
    "#respectful nap time \n",
    "    wait_time=randint(1,4)\n",
    "    print(\"i will sleep for...\" +str(wait_time) +\" seconds now\")\n",
    "    sleep(wait_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(pages[0].content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you print the object pages after running the code above, you'll just see the response code messages, but the html code is still accessible and you can parse it the same way as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build code to collect the relevant information from the Request \n",
    "\n",
    "this is what we need : \n",
    "\n",
    "##### Parse just the first page, for testing purposes\n",
    "- soup=BeautifulSoup(pages[0].content, \"html.parser\")\n",
    "\n",
    "##### title and synopsis\n",
    "\n",
    "- soup.select(\"div.lister-item-content > h3 > a\")\n",
    "- soup.select(\"div.lister-item-content > p:nth-child(4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse just the first page, for testing purposes\n",
    "\n",
    "soup=BeautifulSoup(pages[0].content,\"html.parser\")\n",
    "\n",
    "# Paste the Selector from the first movie title copied from Chrome Dev Tools\n",
    "\n",
    "soup.select(\"#main > div > div.lister.list.detail.sub-list > div > div:nth-child(1) > div.lister-item-content > h3 > a\")\n",
    "\n",
    "# Trim the selection\n",
    "soup.select(\"h3 > a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the Selector from the first movie title copied from Chrome Dev Tools\n",
    "soup.select(\"p:nth-child(4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine all the code \n",
    "\n",
    "There are many approaches to do this. The one we'll follow is: \n",
    "\n",
    "- Loop through the pages we collected, parse them (\"create the soup\") and store the parsed pages in a list. \n",
    "\n",
    "- For each parsed page, select the \"blocks of HTML elements\" that contain all the information of each movie (the title, the synopsis and other stuff). \n",
    "\n",
    "- For each one of the \"blocks\" we collected in the previous step: \n",
    "\n",
    "    - Get the movie titles and store them in a list \n",
    "\n",
    "    - Get the synopsis and store them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles =[]\n",
    "synopsis=[]\n",
    "pages_parsed=[]\n",
    "\n",
    "for i in range(len(pages)):\n",
    "    pages_parsed.append(BeautifulSoup(pages[i].content,\"html.parser\"))\n",
    "    movies_html=pages_parsed[i].select(\"div.lister-item-content\")\n",
    "    #for each movie, store title and synopsis into the lists\n",
    "    for j in range(len(movies_html)):\n",
    "        titles.append(movies_html[j].select(\"h3 > a\")[0].get_text())\n",
    "        synopsis.append(movies_html[j].select(\"p:nth-child(4)\")[0].get_text().strip())\n",
    "\n",
    "# check output\n",
    "print(len(titles))\n",
    "print(len(synopsis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the output and identify any wrangling steps you missed\n",
    "# next, you would create the data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd example - Scraping presidents\n",
    "\n",
    "Our objective is to create a dataframe with information about the presidents of the United States. To do this, we will go through 5 steps:\n",
    "\n",
    "1. Scrape this [list of presidents of the United States](https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import libraries\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# 2. find url and store it in a variable\n",
    "url=\"https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States\"\n",
    "\n",
    "# 3. download html with a get request\n",
    "response=requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "# 4.1. parse html (create the 'soup')\n",
    "soup=BeautifulSoup(response.content,\"html.parser\")\n",
    "\n",
    "# 4.2. check that the html code looks like it should\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Collect all the links to the Wikipedia page of each president.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this solution is not very elegant, but works. \n",
    "# The CSS selector we copied had an \"nth-child\" that we could iterate \n",
    "# to find presidents, but some elements were empty, so we concatenate \n",
    "# each new element with \"+\" instead of appending as usual:\n",
    "\n",
    "presidents=[]\n",
    "\n",
    "for i in range(84):\n",
    "    presidents=presidents + soup.select(\"tbody > tr:nth-child(\" +str(i)+ \") > td:nth-child(4) > b > a\")\n",
    "\n",
    "presidents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can access the links searching for the attribute \"href\"\n",
    "# in each element\n",
    "presidents[45][\"href\"]\n",
    "#https://en.wikipedia.org/wiki/Richard_Nixon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we just assemble a new request to the link\n",
    "url=\"https://en.wikipedia.org\"+presidents[0][\"href\"]\n",
    "\n",
    "# send request\n",
    "response=requests.get(url)\n",
    "response.status_code\n",
    "# parse & store html\n",
    "soup=BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"table\", {\"class\":\"infobox vcard\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scrape the Wikipedia page of each president.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we could very well store the whole wikipedia page for each president, or just the tiny, final pieces of information. Storing the boxes is a middle ground (we don't have too much noise but retain the flexibility of deciding later which specific elements to extract).\n",
    "\n",
    "When sending multiple requests, remember to be respectful by spacing the requests a few seconds from each other. We will also ping the success code to monitor that everything is going well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. find url and store it in a variable\n",
    "\n",
    "presi_soups=[]\n",
    "\n",
    "for presi in presidents:\n",
    "    # send request\n",
    "    url=\"https://en.wikipedia.org\"+presi[\"href\"]\n",
    "    response=requests.get(url)\n",
    "    print(presi.get_text(), response.status_code)\n",
    "    # parse & store html\n",
    "    soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "    presi_soups.append(soup.find(\"table\", {\"class\":\"infobox vcard\"}))\n",
    "    # respectful nap:\n",
    "    wait_time=randint(1,2)\n",
    "    print(\"I will sleep now for...\"+str(wait_time)+\"secs\")\n",
    "    sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find and store information about each president.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extracted the 'infoboxes': now it's time to extract specific information from them. First test what can we get from a single president and then assemble a loop for all of them.\n",
    "\n",
    "Here, we will use [the string argument](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-string-argument) in the find function, since wikipedia tags and classes are not always helpful to locate. The string argument allows us to locate elements by its actual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Birthday\n",
    "presi_soups[45].find(\"span\",{\"class\":\"bday\"}).get_text()\n",
    "\n",
    "#Political party\n",
    "presi_soups([45].find(\"th\",string=\"Political party\").parent.find(\"a\").get_text()\n",
    "\n",
    "#Number of sons/daughters\n",
    "len(presi_soups[45].find(\"th\",string=\"Children\").parent.find_all(\"li\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect with a loop - presidents name, their birthday , political party, no of children \n",
    "name=[]\n",
    "dob=[]\n",
    "party=[]\n",
    "children=[]\n",
    "\n",
    "for presi in presi_soups:\n",
    "    name.append(presi.find(\"div\",{\"class\":\"fn\"}).get_text())\n",
    "    dob.append(presi.find(\"span\",{\"class\":\"bday\"}).get_text())\n",
    "    party.append(presi.find(\"th\",string=\"Political party\").parent.find(\"a\").get_text())\n",
    "    try:\n",
    "        children.append(len(presi.find(\"th\",string=\"Children\").parent.find_all(\"li\")))\n",
    "    except:\n",
    "        children.append(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Organize the information in a dataframe where we have each president as a row and each variable we collected as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents_data= pd.DataFrame({\"name\":name, \"birthday\":dob, \"party\":party, \"noofchildren\":children} )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
